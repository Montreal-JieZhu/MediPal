{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb877d9c",
   "metadata": {},
   "source": [
    "## MediPal -- LLM test and select"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e7cfa1",
   "metadata": {},
   "source": [
    "### In this section, I set three tasks to test three small LLMs' performance, so that I can understand them and design the application properly.\n",
    "##### Tasks:\n",
    "1. Reasoning\n",
    "2. Generate questions based on content\n",
    "3. Solve Yes or No question\n",
    "4. Structured ouput\n",
    "\n",
    "##### Target LLMs:\n",
    "1. meta-llama/Llama-3.2-1B-Instruct \n",
    "2. meta-llama/Meta-Llama-3-8B-Instruct\n",
    "3. ContactDoctor/Bio-Medical-Llama-3-8B\n",
    "\n",
    "##### Why test Bio-Medical-Llama-3-8B？\n",
    "\n",
    "Bio-Medical-Llama-3-8B model is a specialized large language model designed for biomedical applications. It is finetuned from the meta-llama/Meta-Llama-3-8B-Instruct model using a custom dataset containing over 500,000 diverse entries. These entries include a mix of synthetic and manually curated data, ensuring high quality and broad coverage of biomedical topics.\n",
    "\n",
    "The model is trained to understand and generate text related to various biomedical fields, making it a valuable tool for researchers, clinicians, and other professionals in the biomedical domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c7f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e0a11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_dtype():\n",
    "    \"\"\"Return the best dtype for the device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.is_bf16_supported():\n",
    "            return torch.bfloat16\n",
    "        else:\n",
    "            return torch.float16\n",
    "        \n",
    "    return torch.float32\n",
    "\n",
    "def best_device():\n",
    "    \"\"\"Return the device type\"\"\"\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def login_huggingface():\n",
    "    \"\"\"Login HaggingFace\"\"\"\n",
    "    load_dotenv()\n",
    "    login(os.getenv(\"HUGGINGFACE_KEY\"))\n",
    "    print(\"Login HuggingFace!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4974779f",
   "metadata": {},
   "source": [
    "##### Task1: Reasoning\n",
    "##### Prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_last_letters_basic = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a precise but brief problem-solver. \"\n",
    "     \"Explain your reasoning in a few short steps labeled 'Step 1:', 'Step 2:', 'Step 3:' etc., \"\n",
    "     \"then end with a single line 'Result: <value>'. Avoid long explanations.\"),\n",
    "    (\"human\",\n",
    "     \"Given three English words: {w1}, {w2}, {w3}.\\n\"\n",
    "     \"Task:\\n\"\n",
    "     \"- For each word, ignore trailing spaces/punctuation and find the last alphabetic letter (A–Z/a–z).\\n\"\n",
    "     \"- Concatenate these three letters in the SAME order (word1 → word2 → word3).\\n\"\n",
    "     \"Output:\\n\"\n",
    "     \"- Write step-by-step text (Step 1/2/3...).\\n\"\n",
    "     \"- End with one final line: Result: <concatenated_string>.\")\n",
    "])\n",
    "\n",
    "prompt_simple_math = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a precise but brief math tutor. \"\n",
    "     \"Explain in short steps labeled 'Step 1:', 'Step 2:', ... \"\n",
    "     \"then finish with a single line 'Result: <number>'.\"),\n",
    "    (\"human\",\n",
    "     \"John has 5 apples. He gives 2 apples to his father. \"\n",
    "     \"How many apples does John have left?\\n\"\n",
    "     \"Solve step by step and end with: Result: <number>.\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb4252",
   "metadata": {},
   "source": [
    "##### Task2: Generate questions based on content\n",
    "##### Prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae6746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_generate_questions = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are a cautious medical student who generates clinically relevant, self-contained questions. \"\n",
    "     \"Ground every question strictly in the provided document; do not invent details or rely on outside knowledge.\"),\n",
    "    (\"user\",\n",
    "     \"Instructions:\\n\"\n",
    "     \"1) From the document below, write exactly 3 unique QUESTIONS in English only.\\n\"\n",
    "     \"2) Cover different medical perspectives (aim for breadth), such as: symptoms/signs; diagnosis/differential; \"\n",
    "     \"investigations/labs/imaging; treatment/procedures; medications (dose, interactions, adverse effects); \"\n",
    "     \"contraindications/precautions; risk factors/prognosis; prevention/patient counseling; special populations \"\n",
    "     \"(e.g., pregnancy, breastfeeding, pediatrics, geriatrics).\\n\"\n",
    "     \"3) Each question must be concise (≤ 25 words), self-contained (avoid pronouns like 'it/this/that'), and \"\n",
    "     \"directly supported by the document.\\n\"\n",
    "     \"4) Do NOT provide any answers or explanations.\\n\"\n",
    "     \"5) Return ONLY a JSON list that matches exactly this schema: [\\\"question1\\\", \\\"question2\\\", ..., \\\"question3\\\"]\\n\"\n",
    "     \"6) Enclose the JSON list between <json> and </json> tags.\\n\"\n",
    "     \"7) If the document does not support 3 distinct perspectives, still produce 3 questions but avoid near-duplicates; \"\n",
    "     \"prefer covering as many perspectives as possible.\"),\n",
    "    (\"user\", \"Document:\\n{doc}\\n\")\n",
    "])\n",
    "\n",
    "contents = [\n",
    "    \"phenylephrine is used to relieve nasal discomfort caused by colds, allergies, and hay fever. it is also used to relieve sinus congestion and pressure. phenylephrine will relieve symptoms but will not treat the cause of the symptoms or speed recovery. phenylephrine is in a class of medications called nasal decongestants. it works by reducing swelling of the blood vessels in the nasal passages.about Phenylephrine\",\n",
    "    \"Sinner's physical struggles appeared to begin late in the second set, and he rushed to place ice towels around his neck at the changeover before the start of the third. In the decider, the Italian was limping between points and frequently massaged his right thigh. At the 2-1 changeover, he didn't sit and instead put his legs up on his bench to try and ward off cramp.\",\n",
    "    \"Artificial intelligence (AI) investing is still the dominant theme in the stock market. This checks out, as it's where a massive amount of capital is getting invested to build out computing infrastructure and train models. Although it seems like AI has been the prevailing market theme for some time, there are multiple indications that this will persist for several more years, making AI a great place to invest today.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9301b",
   "metadata": {},
   "source": [
    "##### Task 3 and 4: Solve Yes or No question\n",
    "##### Prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_self_contained = PromptTemplate(\n",
    "        template=\"\"\"You are a grader for a question.\n",
    "    You must decide whether the question is self-contained—meaning that it is clear, meaningful, and understandable on its own, without any conversation history or external context.\n",
    "    Here is the user's question: {question} \\n\n",
    "    Return a binary judgment as a JSON object with a single key \"score\".\n",
    "    Respond only with {{\"score\": \"yes\"}} if the question is self-contained,\n",
    "    or {{\"score\": \"no\"}} if it is not. Do not include any explanation or extra text.\"\"\",\n",
    "        input_variables=[\"question\"],\n",
    ")\n",
    "\n",
    "self_contained_questions = [\n",
    "        \"What is the tallest mountain in South America?\",    \n",
    "        \"Can you think about it?\"    \n",
    "        \"Can you explain how blockchain technology works?\",    \n",
    "        \"Do you have a medicine to relieve sinus congestion and pressure?\",        \n",
    "        \"How can I take it?\"  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_related_question = PromptTemplate(\n",
    "        template=\"\"\"You are a conversation coherence grader.\n",
    "        Your task is to decide whether the user's latest message is logically and topically connected to the previous conversation.\n",
    "\n",
    "        Conversation history:\n",
    "        {document}\n",
    "\n",
    "        User's latest message:\n",
    "        {question}\n",
    "\n",
    "        Return only a JSON object with a single key \"score\":\n",
    "        - {{\"score\": \"yes\"}} if the latest message is coherent and contextually related to the conversation history.\n",
    "        - {{\"score\": \"no\"}} if it is not related or breaks the context.\n",
    "\n",
    "        No explanation or extra text.\"\"\",\n",
    "        input_variables=[\"document\", \"question\"],\n",
    "    )  \n",
    "\n",
    "documents = [\"\"\"AI: phenylephrine comes as a tablet, a liquid, or a dissolving strip to take by mouth. it is usually taken every 4 hours as needed.\\n \n",
    "             HUMAN: What form does Phenylephrine come? \"\"\",\n",
    "             \"\"\"Sinner's physical struggles appeared to begin late in the second set, and he rushed to place ice towels around his neck at the changeover\"\"\"]  \n",
    "\n",
    "related_questions = [\"how can I take it?\",\n",
    "             \"what did happen to Sinner?\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64956071",
   "metadata": {},
   "source": [
    "##### First of all, Testing meta-llama/Llama-3.2-1B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b91a20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd542a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            dtype = best_dtype(),\n",
    "            device_map={\"\":best_device()}, \n",
    "            low_cpu_mem_usage=True           \n",
    "        )\n",
    "\n",
    "print(f\"Load {model_id} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13058dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,   \n",
    "    )\n",
    "\n",
    "# Wrapper normal piple with huggingfacepipeline\n",
    "hug_pipe = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Transfer the pipeline to Chat mode.\n",
    "# Because this is a way we can use ChatPromptTemplate to make better prompt.\n",
    "Llama_1b = ChatHuggingFace(llm=hug_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d67af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning test\n",
    "reason_letters_chain = prompt_last_letters_basic | Llama_1b\n",
    "result = reason_letters_chain.invoke({})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacee4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_math_chain = prompt_simple_math | Llama_1b\n",
    "result = reason_math_chain.invoke({})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a3822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions based on content\n",
    "generate_question_chain = prompt_generate_questions | Llama_1b\n",
    "\n",
    "for c in contents:\n",
    "    result = generate_question_chain.invoke({\"doc\": c})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b996f374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Yes or No question， structured_output\n",
    "self_contained_chain = prompt_self_contained | Llama_1b\n",
    "for q in self_contained_questions:\n",
    "    result = self_contained_chain.invoke({\"question\": q})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_chain = prompt_related_question | Llama_1b\n",
    "for q, d in zip(related_questions, documents):\n",
    "    result  = related_chain.invoke({\"question\": q, \"document\": d})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e215e1",
   "metadata": {},
   "source": [
    "##### Testing meta-llama/Meta-Llama-3-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2043988",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cf7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            dtype = best_dtype(),\n",
    "            device_map={\"\":best_device()}, \n",
    "            low_cpu_mem_usage=True           \n",
    "        )\n",
    "\n",
    "print(f\"Load {model_id} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10ac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,   \n",
    "    )\n",
    "\n",
    "# Wrapper normal piple with huggingfacepipeline\n",
    "hug_pipe = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Transfer the pipeline to Chat mode.\n",
    "# Because this is a way we can use ChatPromptTemplate to make better prompt.\n",
    "Llama_8b = ChatHuggingFace(llm=hug_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d19e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning test\n",
    "reason_letters_chain = prompt_last_letters_basic | Llama_8b\n",
    "result = reason_letters_chain.invoke({})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d2d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_math_chain = prompt_simple_math | Llama_8b\n",
    "result = reason_math_chain.invoke({})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75927e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions based on content\n",
    "generate_question_chain = prompt_generate_questions | Llama_8b\n",
    "\n",
    "for c in contents:\n",
    "    result = generate_question_chain.invoke({\"doc\": c})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Yes or No question， structured_output\n",
    "self_contained_chain = prompt_self_contained | Llama_8b\n",
    "for q in self_contained_questions:\n",
    "    result = self_contained_chain.invoke({\"question\": q})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff2eaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_chain = prompt_related_question | Llama_8b\n",
    "for q, d in zip(related_questions, documents):\n",
    "    result  = related_chain.invoke({\"question\": q, \"document\": d})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d3e647",
   "metadata": {},
   "source": [
    "##### Testing ContactDoctor/Bio-Medical-Llama-3-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a13873",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ContactDoctor/Bio-Medical-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c7c054",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            dtype = best_dtype(),\n",
    "            device_map={\"\":best_device()}, \n",
    "            low_cpu_mem_usage=True           \n",
    "        )\n",
    "\n",
    "print(f\"Load {model_id} done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d58f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,   \n",
    "    )\n",
    "\n",
    "# Wrapper normal piple with huggingfacepipeline\n",
    "hug_pipe = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Transfer the pipeline to Chat mode.\n",
    "# Because this is a way we can use ChatPromptTemplate to make better prompt.\n",
    "Medical_Llama_8b = ChatHuggingFace(llm=hug_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reasoning test\n",
    "reason_letters_chain = prompt_last_letters_basic | Medical_Llama_8b\n",
    "result = reason_letters_chain.invoke({})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ca7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reason_math_chain = prompt_simple_math | Medical_Llama_8b\n",
    "result = reason_math_chain.invoke({})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b081bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions based on content\n",
    "generate_question_chain = prompt_generate_questions | Medical_Llama_8b\n",
    "\n",
    "for c in contents:\n",
    "    result = generate_question_chain.invoke({\"doc\": c})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f05ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve Yes or No question， structured_output\n",
    "self_contained_chain = prompt_self_contained | Medical_Llama_8b\n",
    "for q in self_contained_questions:\n",
    "    result = self_contained_chain.invoke({\"question\": q})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f1210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "related_chain = prompt_related_question | Medical_Llama_8b\n",
    "for q, d in zip(related_questions, documents):\n",
    "    result  = related_chain.invoke({\"question\": q, \"document\": d})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFT_QLoRA_Llama3_1B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
