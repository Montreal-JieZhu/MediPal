{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee57847",
   "metadata": {},
   "source": [
    "### In this section, I will involve a LLM as an question-generator to make hypothetical questions based on the docments\n",
    "#### That is one of reasons why I call the RAG system as ***Agentic-RAG***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82302bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid\n",
    "import os\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf38d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will load the json file to json object\n",
    "def load_json_list(path: str):    \n",
    "    with open(path, mode = \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96717523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/datasets/chuncked_data.json\n"
     ]
    }
   ],
   "source": [
    "workspace_base_path = os.getcwd()\n",
    "dataset_path = os.path.join(workspace_base_path, \"datasets\", \"chuncked_data.json\") \n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c68cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json_list(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d5f9654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': '1bf5880b-93ec-4ac9-a0cb-eb35693ccce4',\n",
       "  'questions': ['why is Phenylephrine prescribed?'],\n",
       "  'docs': ['phenylephrine is used to relieve nasal discomfort caused by colds, allergies, and hay fever. it is also used to relieve sinus congestion and pressure. phenylephrine will relieve symptoms but will not treat the cause of the symptoms or speed recovery. phenylephrine is in a class of medications called nasal decongestants. it works by reducing swelling of the blood vessels in the nasal passages.about Phenylephrine'],\n",
       "  'original_doc': 'phenylephrine is used to relieve nasal discomfort caused by colds, allergies, and hay fever. it is also used to relieve sinus congestion and pressure. phenylephrine will relieve symptoms but will not treat the cause of the symptoms or speed recovery. phenylephrine is in a class of medications called nasal decongestants. it works by reducing swelling of the blood vessels in the nasal passages.about Phenylephrine'},\n",
       " {'doc_id': 'f7ad6ffd-7176-4c1d-a7ee-bec020443c2c',\n",
       "  'questions': ['how should Phenylephrine be used?'],\n",
       "  'docs': ['phenylephrine comes as a tablet, a liquid, or a dissolving strip to take by mouth. it is usually taken every 4 hours as needed. follow the directions on your prescription label or the package label carefully, and ask your doctor or pharmacist to explain any part you do not understand. take phenylephrine exactly as directed. do not take more or less of it or take it more often than prescribed by your doctor or directed on the label.phenylephrine comes alone and in combination with other',\n",
       "   'and in combination with other medications. ask your doctor or pharmacist for advice on which product is best for your symptoms. check nonprescription cough and cold product labels carefully before using two or more products at the same time. these products may contain the same active ingredient(s) and taking them together could cause you to receive an overdose. this is especially important if you will be giving cough and cold medications to a child.nonprescription cough and cold combination',\n",
       "   'cough and cold combination products, including products that contain phenylephrine, can cause serious side effects or death in young children. do not give these products to children younger than 4 years of age. if you give these products to children 4 to 11 years of age, use caution and follow the package directions carefully.if you are giving phenylephrine or a combination product that contains phenylephrine to a child, read the package label carefully to be sure that it is the right product',\n",
       "   \"that it is the right product for a child of that age. do not give phenylephrine products that are made for adults to children.before you give a phenylephrine product to a child, check the package label to find out how much medication the child should receive. give the dose that matches the child's age on the chart. ask the child's doctor if you don't know how much medication to give the child.if you are taking the liquid, do not use a household spoon to measure your dose. use the measuring\",\n",
       "   'your dose. use the measuring spoon or cup that came with the medication or use a spoon made especially for measuring medication.if your symptoms do not get better within 7 days or if you have a fever, stop taking phenylephrine and call your doctor.if you are taking the dissolving strips, place one strip on your tongue and allow it to dissolve.about Phenylephrine'],\n",
       "  'original_doc': \"phenylephrine comes as a tablet, a liquid, or a dissolving strip to take by mouth. it is usually taken every 4 hours as needed. follow the directions on your prescription label or the package label carefully, and ask your doctor or pharmacist to explain any part you do not understand. take phenylephrine exactly as directed. do not take more or less of it or take it more often than prescribed by your doctor or directed on the label.phenylephrine comes alone and in combination with other medications. ask your doctor or pharmacist for advice on which product is best for your symptoms. check nonprescription cough and cold product labels carefully before using two or more products at the same time. these products may contain the same active ingredient(s) and taking them together could cause you to receive an overdose. this is especially important if you will be giving cough and cold medications to a child.nonprescription cough and cold combination products, including products that contain phenylephrine, can cause serious side effects or death in young children. do not give these products to children younger than 4 years of age. if you give these products to children 4 to 11 years of age, use caution and follow the package directions carefully.if you are giving phenylephrine or a combination product that contains phenylephrine to a child, read the package label carefully to be sure that it is the right product for a child of that age. do not give phenylephrine products that are made for adults to children.before you give a phenylephrine product to a child, check the package label to find out how much medication the child should receive. give the dose that matches the child's age on the chart. ask the child's doctor if you don't know how much medication to give the child.if you are taking the liquid, do not use a household spoon to measure your dose. use the measuring spoon or cup that came with the medication or use a spoon made especially for measuring medication.if your symptoms do not get better within 7 days or if you have a fever, stop taking phenylephrine and call your doctor.if you are taking the dissolving strips, place one strip on your tongue and allow it to dissolve.about Phenylephrine\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46ec4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e80bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Huggingface so that we can access the model\n"
     ]
    }
   ],
   "source": [
    "with open(\"keys.txt\") as f:\n",
    "    os.environ[\"HF_TOKEN\"] = f.read().strip()\n",
    "\n",
    "# login using env var\n",
    "login(os.environ[\"HF_TOKEN\"])\n",
    "\n",
    "print(f\"Login Huggingface so that we can access the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f90976",
   "metadata": {},
   "source": [
    "### Now I need to use a local LLM to generate hypothetical question for the content.\n",
    "\n",
    "#### I tested three models in my test notebook(That's a mess. I will never show it to you). Result is showing Below:\n",
    "\n",
    "#### Task 1: Generate hypothetical questions for a document and output JSON\n",
    "* meta-llama/Llama-3.2-1B-Instruct did OK job. very fast.\n",
    "* meta-llama/Meta-Llama-3-8B-Instruct is out of control no matter how I prompt it. very slow!\n",
    "* ContactDoctor/Bio-Medical-Llama-3-8B did amazing job. fast enought.\n",
    "\n",
    "#### Task 2: Summarize the document\n",
    "* meta-llama/Llama-3.2-1B-Instruct did OK job. very fast.\n",
    "* meta-llama/Meta-Llama-3-8B-Instruct is out of control. very slow!\n",
    "* ContactDoctor/Bio-Medical-Llama-3-8B did amazing job. fast enought.\n",
    "\n",
    "#### So I have no any reason not to select ContactDoctor/Bio-Medical-Llama-3-8B\n",
    "\n",
    "#### As the model provider's citation:\n",
    "\n",
    "@misc{ContactDoctor_Bio-Medical-Llama-3-8B, author = ContactDoctor, title = {ContactDoctor-Bio-Medical: A High-Performance Biomedical Language Model}, year = {2024}, howpublished = {https://huggingface.co/ContactDoctor/Bio-Medical-Llama-3-8B}, }\n",
    "\n",
    "Bio-Medical-Llama-3-8B model is a specialized large language model designed for biomedical applications. It is finetuned from the meta-llama/Meta-Llama-3-8B-Instruct model using a custom dataset containing over 500,000 diverse entries. These entries include a mix of synthetic and manually curated data, ensuring high quality and broad coverage of biomedical topics.\n",
    "\n",
    "The model is trained to understand and generate text related to various biomedical fields, making it a valuable tool for researchers, clinicians, and other professionals in the biomedical domain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d8a0a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ContactDoctor/Bio-Medical-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ffba088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc63c2dd",
   "metadata": {},
   "source": [
    "#### Better quantization will speed up the inference time extrordinately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6b8a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_dtype():\n",
    "    if torch.cuda.is_available():\n",
    "        if torch.cuda.is_bf16_supported():\n",
    "            return torch.bfloat16\n",
    "        else:\n",
    "            return torch.float16\n",
    "        \n",
    "    return torch.float32\n",
    "\n",
    "def best_device():\n",
    "    return \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50e5937d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "print(best_dtype())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd2b9e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ContactDoctor/Bio-Medical-Llama-3-8B works best at bfloat16 quantization.\n",
    "# In order to load a lighter LLM and still don't lose the performance.\n",
    "# I will load it in 8bit and keep the dtype as bfloat16. This is a balance choise.\n",
    "def load_model(model_id: str):\n",
    "    bnb_cfg = None\n",
    "\n",
    "    if best_device() == \"cuda\":\n",
    "        bnb_cfg = BitsAndBytesConfig(\n",
    "            load_in_8bit=True, # I don't want to lost LLM's performance     \n",
    "            load_in_8bit_fp32_cpu_offload=False,\n",
    "        )\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config = bnb_cfg,\n",
    "            dtype = best_dtype(),\n",
    "            device_map = \"auto\",                        \n",
    "        )\n",
    "        print(f\"The bnb configuration: {bnb_cfg}\")\n",
    "    else: #CPU\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            dtype = best_dtype(),\n",
    "            device_map={\"\":best_device()}, \n",
    "            low_cpu_mem_usage=True           \n",
    "        )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0560185f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912ca6d20b3d461d97c7dc9277b00b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bnb configuration: BitsAndBytesConfig {\n",
      "  \"_load_in_4bit\": false,\n",
      "  \"_load_in_8bit\": true,\n",
      "  \"bnb_4bit_compute_dtype\": \"float32\",\n",
      "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "  \"bnb_4bit_quant_type\": \"fp4\",\n",
      "  \"bnb_4bit_use_double_quant\": false,\n",
      "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "  \"llm_int8_has_fp16_weight\": false,\n",
      "  \"llm_int8_skip_modules\": null,\n",
      "  \"llm_int8_threshold\": 6.0,\n",
      "  \"load_in_4bit\": false,\n",
      "  \"load_in_8bit\": true,\n",
      "  \"quant_method\": \"bitsandbytes\"\n",
      "}\n",
      "\n",
      "Load tokenizer done!\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = load_model(model_id)\n",
    "\n",
    "print(\"Load tokenizer done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9b47dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": false,\n",
      "    \"_load_in_8bit\": true,\n",
      "    \"bnb_4bit_compute_dtype\": \"float32\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"fp4\",\n",
      "    \"bnb_4bit_use_double_quant\": false,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": false,\n",
      "    \"load_in_8bit\": true,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "ContactDoctor/Bio-Medical-Llama-3-8B\n"
     ]
    }
   ],
   "source": [
    "print(model)                    # full architecture tree (long but useful)\n",
    "print(model.config)             # core hyperparameters (dims, layers, headsâ€¦)\n",
    "print(model.name_or_path)       # the checkpoint id/path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "650ed871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,   \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b274a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "hug_pipe = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "587d1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generator = ChatHuggingFace(llm=hug_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1751e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM model is ready. \n",
    "# Here is the definition of the structure of output schema\n",
    "class QuestionList(BaseModel):\n",
    "    questions: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"A list of unique, concise questions in English; each ends with a question mark.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab1b3dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#structured_generator = question_generator.with_structured_output(QuestionList)\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=QuestionList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e829e560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt engineering can guide the LLM to generate output we expect.\n",
    "\n",
    "# def prompt_maker(tok, doc_text: str, n: int) -> str:\n",
    "\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": \"You are a medical student. You like ask questions when you read clinical documents.\"},\n",
    "#         {\"role\": \"user\", \"content\": (\n",
    "#         f\"Read the document and generate {n} clinically relevant questions.\\n\"    \n",
    "#         \"Requirements:\\n\" \n",
    "#         \"1) The questions you generate should be from different medical perspectives.\\n\"\n",
    "#         \"2) Don't produce answers. Questions only.\\n\"\n",
    "#         \"3) Output only JSON format, for example: {'questions:': ['question1','question2',...,'question5']}.\\n\"\n",
    "#         f\"Document:\\n{doc_text}\\n\\n\"\n",
    "#         \"JSON:\"\n",
    "#         )}\n",
    "#     ]\n",
    "\n",
    "#     prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "#     return prompt\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \n",
    "     \"You are a medical student. You like ask questions when you read clinical documents.\"),\n",
    "    (\"user\", \n",
    "     \"1.From the document below, write exactly {n} unique QUESTIONS only. \"\n",
    "     \"2.The questions you generate should be from different medical perspectives. \"\n",
    "     \"3.Don't generate answer. \"\n",
    "     \"4.Output JSON format, for example: {{\\\"questions\\\": [\\\"question1\\\",\\\"question2\\\",...,\\\"question5\\\"]}}.\"),    \n",
    "    (\"user\", \"Document:\\n{doc}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e0c9639b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['doc', 'n'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a medical student. You like ask questions when you read clinical documents.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['n'], input_types={}, partial_variables={}, template='1.From the document below, write exactly {n} unique QUESTIONS only. 2.The questions you generate should be from different medical perspectives. 3.Don\\'t generate answer. 4.Output JSON format, for example: {{\"questions\": [\"question1\",\"question2\",...,\"question5\"]}}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['doc'], input_types={}, partial_variables={}, template='Document:\\n{doc}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c83933f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple chain which will feed a prompt to the generator to produce stucture json object with questions\n",
    "chain = prompt | question_generator | parser  # returns a QuestionList object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "317eae2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/miniconda3/envs/ai311/lib/python3.11/site-packages/bitsandbytes/autograd/_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Is phenylephrine generally safe for use in pediatric population?', 'How does phenylephrine affect blood vessels?', 'Is phenylephrine contraindicated in patients with narrow-angle glaucoma?']\n"
     ]
    }
   ],
   "source": [
    "doc = \"phenylephrine is used to relieve nasal discomfort... (your text)\"\n",
    "n = 3\n",
    "res: QuestionList = chain.invoke({\n",
    "    \"n\": n,\n",
    "    \"doc\": doc,\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "print(res.questions)  # already parsed & validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529a167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide how many questions it need to generate based on the length of the document.\n",
    "# From my experience, 500 to 2000 : 5 questions, 100 to 500 : 3 questions, less than 100 : 1 question.\n",
    "# But it can be varied.\n",
    "def decide_n_questions(txt: str):\n",
    "    wc = len(txt.split())\n",
    "    if wc > 500:\n",
    "        return 5\n",
    "    elif wc > 100:\n",
    "        return 3\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c7b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's iterate all medicine's docments\n",
    "\n",
    "for content in data:\n",
    "    for doc in content[\"docs\"]:\n",
    "        n = decide_n_questions(doc)\n",
    "        questions_list = chain.invoke({\"n\": n, \"doc\": doc})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
