{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95707416",
   "metadata": {},
   "source": [
    "## MediPal -- Data Source ETL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f892cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "import warnings\n",
    "import langid\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85947090",
   "metadata": {},
   "source": [
    "### In this section, I am building a data pipeline to scrape medicine information from datasource. At this stage, I only use one public source.\n",
    "\n",
    "The pipeline will explore the website from base to child pages. It will go deeply at third layers to get the drugs data.\n",
    "\n",
    "1. The process will visit the drugs information page to get the urls of drugs name which begin with A to Z.\n",
    "\n",
    "2. Then, the process will iterate exploring each url to get each drug's url and save it to a list.\n",
    "\n",
    "3. Finally, it will extract the drugs' information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1e56850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "base_source_url = \"https://medlineplus.gov\"\n",
    "\n",
    "start_source_url = urljoin(base_source_url, \"druginformation.html\")\n",
    "\n",
    "header = {\n",
    "     # A real UA helps avoid basic bot filters\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be8c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will iterately extract the urls of page contain the drugs which's name begin with from A to Z\n",
    "def get_az_drug_index_urls():\n",
    "    resp = requests.get(start_source_url, headers=header, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    az_div = soup.find(\"div\", id=\"az-section2\") # All the urls are inside this section\n",
    "    az_urls = []\n",
    "    if az_div:\n",
    "        for a in az_div.find_all(\"a\", href=True):\n",
    "            sub_url = a[\"href\"]\n",
    "            if re.search(r\"druginfo/drug_[A-Z]a\\.html$\", sub_url): # Filter with regex to get exactly url we want\n",
    "                url = urljoin(base_source_url, sub_url)\n",
    "                az_urls.append(url)\n",
    "\n",
    "    return az_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e152073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_to_z_urls = get_az_drug_index_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6a16a249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://medlineplus.gov/druginfo/drug_Aa.html',\n",
       " 'https://medlineplus.gov/druginfo/drug_Ba.html',\n",
       " 'https://medlineplus.gov/druginfo/drug_Ca.html']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_to_z_urls[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "50e14300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_drug_urls(a_to_z_urls):\n",
    "    all_drug_urls = []\n",
    "\n",
    "    for url in a_to_z_urls:\n",
    "        try:\n",
    "            resp = requests.get(url, headers=header, timeout=20)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "            ul_index = soup.find(\"ul\", id=\"index\")\n",
    "            if not ul_index:\n",
    "                continue\n",
    "\n",
    "            for a in ul_index.find_all(\"a\", href=True):\n",
    "                drug_url = urljoin(url, a[\"href\"])\n",
    "                all_drug_urls.append(drug_url)\n",
    "\n",
    "            delay = random.uniform(1, 3)\n",
    "            time.sleep(delay)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {url}: {e}\")\n",
    "\n",
    "    return all_drug_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d1a8057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drug_urls = extract_drug_urls(a_to_z_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4dbb2b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7391"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_drug_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "82624694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://medlineplus.gov/druginfo/meds/a606008.html',\n",
       " 'https://medlineplus.gov/druginfo/meds/a601105.html',\n",
       " 'https://medlineplus.gov/druginfo/meds/a607073.html']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_drug_urls[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6386e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_drugs(all_drug_urls):\n",
    "    \"\"\"\n",
    "    Visit each URL, scrape data, random-sleep between requests, and return a list of dicts.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i, url in enumerate(all_drug_urls, start=1):        \n",
    "        for attempts in range(3):  # Retry 3 times          \n",
    "            resp = requests.get(url, headers=header, timeout=20)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "            # drug name\n",
    "            h1 = soup.find(\"h1\", class_=\"with-also\")\n",
    "            drug_name = h1.get_text(strip=True) if h1 else \"\"\n",
    "            # pronunciation\n",
    "            pron_el = soup.find(\"span\", id=\"d-pronunciation\")\n",
    "            pronunciation = pron_el.get_text(strip=True) if pron_el else \"\"\n",
    "            # sections\n",
    "            subtitles = []            \n",
    "            for head in soup.find_all(\"div\", class_=\"section-header\"):\n",
    "                h2 = head.find(\"div\", class_=\"section-title\")\n",
    "                h2 = h2.find(\"h2\") if h2 else None\n",
    "                title_text = h2.get_text(strip=True) if h2 else None\n",
    "                if title_text.find(\"Brand names\") > -1:\n",
    "                    break\n",
    "                body_div = None\n",
    "                btn = head.find(\"button\", attrs={\"aria-controls\": True})\n",
    "                if btn:\n",
    "                    body_id = btn.get(\"aria-controls\")\n",
    "                    if body_id:\n",
    "                        body_div = soup.find(\"div\", id=body_id, class_=\"section-body\")\n",
    "                if body_div is None:\n",
    "                    body_div = head.find_next_sibling(\"div\", class_=\"section-body\")\n",
    "                if title_text and body_div:\n",
    "                    subtitles.append({\n",
    "                        \"title\": title_text,\n",
    "                        \"content\": str(body_div)\n",
    "                    })\n",
    "\n",
    "            item = {\n",
    "                \"id\": i,\n",
    "                \"drug_name\": drug_name,\n",
    "                \"pronunciation\": pronunciation,\n",
    "                \"url\": url,\n",
    "                \"subtitles\": subtitles  # list of {title, content}\n",
    "            }\n",
    "            out.append(item)\n",
    "            # random delay avoid auti-bot policy\n",
    "            delay = random.uniform(1, 3)                \n",
    "            time.sleep(delay)\n",
    "            break  # success -> exit retry loop         \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c87f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total scraped: 7391\n"
     ]
    }
   ],
   "source": [
    "data = scrape_all_drugs(all_drug_urls)\n",
    "print(f\"Total scraped: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cd8f82",
   "metadata": {},
   "source": [
    "##### We finally got 7391 drugs' info.\n",
    "##### The pipleline took 5 hours to crawl. As I made it randomly sleep 1 - 3 seconds after extract each drug to immitate a human behavior.\n",
    "##### Now I have to save it to my local so that I don't need to scrape again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8866fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(r\"datasets\\medlineplus_drugs.json\")\n",
    "out_path.parent.mkdir(parents=True, exist_ok=True)  # make folder if needed\n",
    "\n",
    "with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFT_QLoRA_Llama3_1B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
