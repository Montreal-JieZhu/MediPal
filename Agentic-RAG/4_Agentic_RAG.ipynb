{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783a234e",
   "metadata": {},
   "source": [
    "### In this section, I will build an Agentic RAG\n",
    "\n",
    "Now I have a Rerank RAG which can retrieve relevant medicine documents based on the query.\n",
    "\n",
    "But why I need an **Agentic RAG**?\n",
    "\n",
    "Becuase Rerank RAG can only similarity search documents by query. If the query contains no relevant information linked to the documents in vector database, it is not able to retrieve relevant docs.\n",
    "\n",
    "I describe some real-life scenarios we can have below:\n",
    "\n",
    "### Problem description:\n",
    "\n",
    "In real conversation, users can ask anything we can not predict ahead. \n",
    "\n",
    "For example:\n",
    "In the third turn the user really want to ask 'How do I take Phenylephrine?'\n",
    "\n",
    "But he types 'How do I take it?'. From the context, 'it' means 'Phenylephrine'.\n",
    "\n",
    "If we retrieve documents by query 'How do I take it?', we can get unrelevant document.  'How do I take Phenylephrine?' makes more sense.\n",
    "\n",
    "Other scenarios:\n",
    "\n",
    "1. In first turn, a user just greet without any question.\n",
    "2. User ask a random question in the middle of conversation.\n",
    "3. .........\n",
    "\n",
    "### Analysis:\n",
    "\n",
    "The root problem is how to determine whether a query is a clinial/medical query and whether a query is related previous conversation.\n",
    "\n",
    "### Solution:\n",
    "\n",
    "#### To handle all those, I will put a local LLM as a master agent to determine what to do next based on different situation.\n",
    "#### So I will involve Basic RAG, langgraph, memory, local LLM, wiki search tool... working together to make the RAG can retrieve real relevant documents by itself.\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "* I will involve an agent to decide what to do next based on the query and history conversation. \n",
    "* Then, the agent will execute the task and observe the result to decide again..... until get a proper result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac03e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# My own libraries\n",
    "from mytools import best_dtype, best_device, login_huggingface\n",
    "from rerank_rag import Rerank_RAG\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import uuid\n",
    "import random\n",
    "import settings\n",
    "\n",
    "from typing import TypedDict, List, Literal, Any, Dict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory # Short-term Memory\n",
    "from langchain_core.messages import BaseMessage\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb9626d",
   "metadata": {},
   "source": [
    "#### As I mentioned previously:\n",
    "\n",
    "Bio-Medical-Llama-3-8B model is a specialized large language model designed for biomedical applications. It is finetuned from the meta-llama/Meta-Llama-3-8B-Instruct model using a custom dataset containing over 500,000 diverse entries. These entries include a mix of synthetic and manually curated data, ensuring high quality and broad coverage of biomedical topics.\n",
    "\n",
    "The model is trained to understand and generate text related to various biomedical fields, making it a valuable tool for researchers, clinicians, and other professionals in the biomedical domain.\n",
    "\n",
    "@misc{ContactDoctor_Bio-Medical-Llama-3-8B, author = ContactDoctor, title = {ContactDoctor-Bio-Medical: A High-Performance Biomedical Language Model}, year = {2024}, howpublished = {https://huggingface.co/ContactDoctor/Bio-Medical-Llama-3-8B}, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8cdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ContactDoctor/Bio-Medical-Llama-3-8B\"\n",
    "\n",
    "cross_encoder_model_id = \"ncbi/MedCPT-Cross-Encoder\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195419ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login HuggingFace!\n"
     ]
    }
   ],
   "source": [
    "login_huggingface() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78220075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2246cca849741af9717be574e2b7e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load tokenizer and base model done!\n"
     ]
    }
   ],
   "source": [
    "# Load a HuggingFace model. Inference it from local GPU.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype = best_dtype(),\n",
    "    device_map={\"\":best_device()},     \n",
    "    low_cpu_mem_usage=True     \n",
    ")\n",
    "print(\"Load tokenizer and base model done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad3ff620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 128009,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model)                    # full architecture tree (long but useful)\n",
    "print(model.config)             # core hyperparameters (dims, layers, headsâ€¦)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da7c35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "original_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,  \n",
    "    temperature=0.1,  \n",
    "    return_full_text=False,   \n",
    ")\n",
    "\n",
    "# Wrapper normal piple with huggingfacepipeline\n",
    "hug_pipeline = HuggingFacePipeline(pipeline=original_pipeline)\n",
    "\n",
    "master_llm = ChatHuggingFace(llm=hug_pipeline) # It is the brain of the whole system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2dffdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_encoder = CrossEncoder(cross_encoder_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c8317",
   "metadata": {},
   "source": [
    "### Master LLM is ready. Next RAG...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3662b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = Rerank_RAG()\n",
    "rag.build_medicine_retriever()\n",
    "wiki = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e85d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a short-term memory\n",
    "\n",
    "class Short_Term_Memory():\n",
    "    def __init__(self) -> None: \n",
    "        \"\"\"Initialize the message container and current session id \"\"\"       \n",
    "        self.session_store: dict[int,BaseChatMessageHistory] = {}\n",
    "        self.current_session_id: int = 0\n",
    "\n",
    "    def get_history(self, session_id: int) -> BaseChatMessageHistory:    \n",
    "        \"\"\"return history messages by sessionId\"\"\"    \n",
    "        self.current_session_id = session_id\n",
    "        if session_id not in self.session_store:\n",
    "            self.session_store[session_id] = ChatMessageHistory()\n",
    "        return self.session_store[session_id]\n",
    "    \n",
    "    def get_current_history(self) -> BaseChatMessageHistory:\n",
    "        \"\"\"return history messages for current session\"\"\"\n",
    "        return self.get_history(self.current_session_id)\n",
    "    \n",
    "    def add_message(self, session_id: int, message: BaseMessage) -> None:\n",
    "        history_messages = self.get_history(session_id)\n",
    "        if len(history_messages.messages) >= 5: # Only keep the recent 5 messages\n",
    "            del history_messages.messages[0] # Remove the first message\n",
    "            history_messages.add_message(message)\n",
    "    \n",
    "    def delete_history(self, session_id: int) -> bool:\n",
    "        \"\"\"delete history messages by sessionId\"\"\"\n",
    "        if session_id in self.session_store:\n",
    "            deleted = self.session_store.pop(session_id)\n",
    "            if deleted:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def delete_current_history(self) -> bool:\n",
    "        \"\"\"delete history messages for current session\"\"\"\n",
    "        return self.delete_history(self.current_session_id)\n",
    "    \n",
    "# Convert a history chat message to a string\n",
    "def history_as_text(history: BaseChatMessageHistory) -> str:\n",
    "    \"\"\"convert history messsages into a string\"\"\"\n",
    "    return \"\\n\".join([\n",
    "        f\"{m.type.upper()}: {m.content}\"   # e.g. \"HUMAN: â€¦\" or \"AI: â€¦\"\n",
    "        for m in history.messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the graph.\n",
    "\n",
    "    Attributes:\n",
    "        session_id: current session id\n",
    "        query: user's query or augmented query\n",
    "        retrieved_doc: retrieval docment    \n",
    "        grade: keep the binary score for every router node to make decision   \n",
    "        wiki_used: Flag whether it already used Wiki search\n",
    "    \"\"\"\n",
    "    session_id: int\n",
    "    query: str\n",
    "    retrieved_doc: str\n",
    "    grade: dict\n",
    "    wiki_used: bool      # Avoid infinity loop in graph\n",
    "    rewrite_counter: int # Avoid infinity loop in graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50cf13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a global short-term memory for all users\n",
    "settings.SHORT_TERM_MEMORY = Short_Term_Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test function\n",
    "\n",
    "def unit_test(action_func, decide_function):\n",
    "    questions = [\n",
    "        \"Why doesn't my friend play tennis with me?\",\n",
    "        \"What is the tallest mountain in South America?\",\n",
    "        \"How does a solar eclipse occur?\",\n",
    "        \"Can you explain how blockchain technology works?\",\n",
    "        \"What are the main differences between classical and jazz music?\",\n",
    "        \"If you could visit any planet in our solar system, which would you choose and why?\",\n",
    "        \"My nasal is disconfort. Do you have a medicine to relieve sinus congestion and pressure?\",\n",
    "        \"What are the common side effects of taking ibuprofen daily?\",\n",
    "        \"Which symptoms usually appear first in a case of seasonal influenza?\",\n",
    "        \"Is it safe to take antihistamines and decongestants at the same time?\",\n",
    "        \"What are the warning signs of a severe allergic reaction?\",\n",
    "        \"How can I tell the difference between a tension headache and a migraine?\"\n",
    "    ]\n",
    "\n",
    "    session_id = 1\n",
    "\n",
    "    for query in questions:\n",
    "        state = AgentState(session_id=session_id,query=query,wiki_used=False)\n",
    "        state = action_func(state)\n",
    "        result = decide_function(state)\n",
    "        print(f\"Decision: {result} \\n\")\n",
    "        print(f\"State: {state} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d5ac26",
   "metadata": {},
   "source": [
    "#### Local and small LLMs usually has no robust structured output. So I have to prepare for all possible results it might output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1fceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_score(text: str) -> str:\n",
    "    \"\"\"Force 'yes'/'no' from messy text.\"\"\"\n",
    "    t = text.strip().lower()\n",
    "    if t in {\"yes\", \"y\", \"true\",\"ok\", \"1\"}:\n",
    "        return \"yes\"\n",
    "    if t in {\"no\", \"n\", \"false\", \"0\"}:\n",
    "        return \"no\"\n",
    "    # heuristics: ambiguous/under-specified â†’ \"no\"\n",
    "    return \"no\"\n",
    "\n",
    "\n",
    "def _extract_json_like(s: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Try hard to find {\"score\": \"...\"} inside messy output.\n",
    "    \"\"\"\n",
    "    # 1) quick regex for a minimal JSON object with score\n",
    "    m = re.search(r'\\{[^{}]*\"score\"\\s*:\\s*\"(?P<score>yes|no|true|false)\"[^{}]*\\}', s, flags=re.I)\n",
    "    if m:\n",
    "        return {\"score\": _normalize_score(m.group(\"score\"))}\n",
    "\n",
    "    # 2) fall back: if the model just said \"yes\"/\"no\" without JSON\n",
    "    yn = re.search(r'\\b(yes|no|true|false)\\b', s, flags=re.I)\n",
    "    if yn:\n",
    "        return {\"score\": _normalize_score(yn.group(1))}\n",
    "\n",
    "    # 3) last resort default\n",
    "    return {\"score\": \"no\"}\n",
    "\n",
    "def robust_binary_grader(prompt: PromptTemplate, query: str, document: str = \"\") ->dict:\n",
    "    \"\"\" \n",
    "    Make sure robustly parse the grade result of Local LLM \n",
    "    \"\"\"\n",
    "    # Base parser (strict JSON with a single key)\n",
    "    base_parser = JsonOutputParser(pydantic_object=None, json_kwargs={\"strict\": False})\n",
    "    # Auto-fixing parser: if model outputs invalid JSON, it asks the LLM to repair\n",
    "    fixing_parser = OutputFixingParser.from_llm(parser=base_parser, llm=master_llm)\n",
    "       # Lower temperature for determinism\n",
    "    chain = prompt | master_llm | fixing_parser\n",
    "    result = None\n",
    "    try:\n",
    "        # First attempt: LLM â†’ (auto-fixing) parser\n",
    "        if document == \"\":\n",
    "            result = chain.invoke({\"question\": query})  \n",
    "        else:\n",
    "            result = chain.invoke({\"question\": query, \"document\": document})        \n",
    "        # result may already be a dict (from parser), but be defensive:\n",
    "        if isinstance(result, dict) and \"score\" in result:\n",
    "            score = _normalize_score(str(result[\"score\"]))\n",
    "            return {\"score\": score}  # exact contract            \n",
    "\n",
    "        # If parser returned a string (some models), try to json-load or extract\n",
    "        if isinstance(result, str):\n",
    "            try:\n",
    "                json_obj = _extract_json_like(result)\n",
    "                score = _normalize_score(str(json_obj.get(\"score\", \"\")))\n",
    "                return {\"score\": score}                \n",
    "            except Exception:\n",
    "                pass\n",
    "        # Fall through to the worst baseline\n",
    "        if any(r in str(result).lower() for r in [\"yes\", \"true\"]):\n",
    "            return {\"score\": \"yes\"}\n",
    "        else:\n",
    "            return {\"score\": \"no\"}       \n",
    "\n",
    "    except Exception as e:\n",
    "        # Hard fallback path if LLM/parse fails entirely\n",
    "        print(f\"[grade_selfcontained_query] Warning: parse failed: {e} \\n\") \n",
    "\n",
    "        return {\"score\": \"no\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ab4df7",
   "metadata": {},
   "source": [
    "#### Local LLMs usually output things without control. So I have to handle with all possible results it might output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756510fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_one_line_question(text: str, fallback: str, max_len: int = 100) -> str:\n",
    "    \"\"\"\n",
    "    Make whatever the LLM returned into a clean single-line question.\n",
    "    - strip code fences, quotes, labels\n",
    "    - collapse whitespace\n",
    "    - take the first question-looking sentence if multiple\n",
    "    - ensure it ends with '?'\n",
    "    - length-limit (soft)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text or \"\")\n",
    "\n",
    "    t = text.strip()\n",
    "\n",
    "    # remove common code fences or labels\n",
    "    t = re.sub(r\"^`{3,}.*?\\n|\\n`{3,}$\", \"\", t, flags=re.S)        # ```...```\n",
    "    t = re.sub(r\"^(re.?written|improved|final|answer)\\s*:\\s*\", \"\", t, flags=re.I)\n",
    "    t = re.sub(r\"^\\\"|\\\"$\", \"\", t)  # trim surrounding quotes\n",
    "    t = re.sub(r\"^'+|'+$\", \"\", t)  # trim surrounding single quotes\n",
    "\n",
    "    # collapse to one line\n",
    "    t = \" \".join(t.split())\n",
    "\n",
    "    # If LLM returned multiple sentences, try to pick the first question-like sentence.\n",
    "    # Prefer the first chunk that ends with '?'\n",
    "    m = re.search(r\"([^?]{3,}\\?)\", t)\n",
    "    if m:\n",
    "        t = m.group(1).strip()\n",
    "\n",
    "    # If still no question mark, try to cut at a sentence boundary and add '?'\n",
    "    if \"?\" not in t:\n",
    "        # take up to first period/exclamation if present, else keep entire\n",
    "        m2 = re.split(r\"[.!]\", t, maxsplit=1)\n",
    "        candidate = m2[0].strip()\n",
    "        # guard against empty\n",
    "        if len(candidate) >= 3:\n",
    "            t = candidate\n",
    "        if not t.endswith(\"?\"):\n",
    "            t = t.rstrip(\"?\") + \"?\"\n",
    "\n",
    "    # truncate softly (avoid cutting mid-word)\n",
    "    if len(t) > max_len:\n",
    "        t = t[:max_len].rsplit(\" \", 1)[0].rstrip(\"?,.;:! \") + \"?\"\n",
    "\n",
    "    # last resort fallback\n",
    "    if len(t) < 3:\n",
    "        t = fallback.strip()\n",
    "        if not t.endswith(\"?\"):\n",
    "            t += \"?\"\n",
    "\n",
    "    return t\n",
    "\n",
    "\n",
    "def robust_question_generater(prompt: PromptTemplate, query: str, document: str = \"\") -> str:\n",
    "    \"\"\" \n",
    "    Make sure robustly extract the question Local LLM generates.\n",
    "    \"\"\"\n",
    "    chain = prompt | master_llm | StrOutputParser()\n",
    "    try:\n",
    "        raw = chain.invoke({\"question\": query, \"document\": document})\n",
    "        result = _clean_one_line_question(raw, fallback=query, max_len=50)\n",
    "        return result\n",
    "    except Exception:\n",
    "        # hard fallback: if model call fails, return original as a question\n",
    "        return query if query.endswith(\"?\") else (query + \"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0200765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiki_to_json(s: str):\n",
    "    \"\"\" \n",
    "    convert the docs from Wikipedia into a list of json objects\n",
    "    \"\"\"\n",
    "    records = [r.strip() for r in s.strip().split(\"\\n\\n\") if r.strip()]\n",
    "\n",
    "    data = []\n",
    "    for record in records:\n",
    "        page_match = re.search(r\"Page:\\s*(.+)\", record)\n",
    "        summary_match = re.search(r\"Summary:\\s*(.+)\", record, re.DOTALL)\n",
    "        if page_match and summary_match:\n",
    "            data.append({\n",
    "                \"Page\": page_match.group(1).strip(),\n",
    "                \"Summary\": summary_match.group(1).strip()\n",
    "            })\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action Node\n",
    "def grade_selfcontained_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Determine whether a query is meaningful, clear, and self-contained\n",
    "    without relying on prior conversation context.    \n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    print(f\"===Step {settings.STEP}. Got a new query: {query}===\\n\")\n",
    "    print(\"I will check if the query is self-contained.\\n\")  \n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader for a question. \\n \n",
    "        You need to determine if a question is meaningful, clear, self-contained without any ambiguity, if you don't know the conversation context. \\n    \n",
    "        Here is the user's question: {question} \\n   \n",
    "        Give a binary score 'yes' or 'no' score to indicate whether the question is meaningful and self-contained. \\n     \n",
    "        Only provide the binary score as a JSON object with a single key 'score', for example {{\"score\": \"yes\"}} or {{\"score\": \"no\"}}. No premable or explanation.\"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    state[\"grade\"] = robust_binary_grader(prompt=prompt, query=query)\n",
    "    settings.STEP += 1\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ee96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Node\n",
    "def decide_selfcontained_query(state: AgentState) -> str:\n",
    "    \"\"\" \n",
    "    If it's a self-contained query, go to grader node for clinical checking.\n",
    "    If it's not a self-contained query, go to grader node for history related checking. \n",
    "    \"\"\"\n",
    "    if state['grade'][\"score\"] == \"yes\":\n",
    "        print(f\"The query is a self-contained one. We don't need to augment it. Let's check if it is a clinical query.\\n\")\n",
    "        return \"grade_clinical\"\n",
    "    else:\n",
    "        print(f\"The query is not self-contained. Let's check if it is related to history conversation. \\n\")\n",
    "        return \"grade_related_history\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_test(grade_selfcontained_query, decide_selfcontained_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66633d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action Node\n",
    "def grade_clinical_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Determine whether a query is about medicine, clinical questions\n",
    "    without relying on prior conversation context.    \n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    print(f\"===Step {settings.STEP}. The query is: {query}===\\n\")\n",
    "    print(\"I will check if the query is about medicine or clinical questions.\\n\")   \n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader for a question.\n",
    "        You need to determine if the user's question is a clinical/medical question.\n",
    "        Consider clinical if it asks about diagnosis, symptoms, treatment, medications (dose, interactions, side effects), test/lab interpretation, procedures, triage (\"should I see a doctor/ER?\"), risks/prognosis, or health advice for humans or animals.\n",
    "        Non-clinical includes general health trivia/news, biology concepts without personal care decisions, admin/insurance/scheduling, or unrelated topics.\n",
    "        Here is the user's question: {question} \\n\n",
    "        Give a binary score 'yes' or 'no' to indicate whether it is a clinical question.\n",
    "        Only provide the binary score as a JSON with a single key 'score', for example {{\"score\": \"yes\"}} or {{\"score\": \"no\"}}.\n",
    "        No preamble or explanation.\"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )    \n",
    "\n",
    "    state[\"grade\"] = robust_binary_grader(prompt=prompt, query=query)\n",
    "    settings.STEP += 1\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Node\n",
    "def decide_clinical_query(state: AgentState) -> str:\n",
    "    \"\"\" \n",
    "    If it is a clinical query and self-contained, go to retrieve node directly.\n",
    "    If it is not a clinical query at all, go to return_sorry node.\n",
    "    \"\"\"\n",
    "    if state['grade'][\"score\"] == \"yes\":\n",
    "        print(f\"The query is a clinical one. We can retrive some documents now.\\n\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(f\"The query is not clinical query. I have nothing to do with it. \\n\")\n",
    "        return \"return_sorry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_test(grade_clinical_query, decide_clinical_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e45ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action Node\n",
    "def grade_history_related_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Determine whether a query is related to history conversations.    \n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    history = settings.SHORT_TERM_MEMORY.get_history(state[\"session_id\"])\n",
    "    history_conversation = history_as_text(history)\n",
    "\n",
    "    print(f\"===Step {settings.STEP}. The query is: {query}===\\n\")\n",
    "    print(\"I will check if the query is related to history conversations. \\n\")   \n",
    "    print(f\"history conversations: {history_conversation} \\n\")   \n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are a grader assessing relevance between the user's current question and history conversation. \\n \n",
    "        Here is current question: {question} \\n\n",
    "        Here is the history conversations: \\n {document} \\n        \n",
    "        Give a binary score 'yes' or 'no' to indicate whether the question is related to history conversations.\n",
    "        Only provide the binary score as a JSON with a single key 'score', for example {{\"score\": \"yes\"}} or  {{\"score\": \"no\"}}.\n",
    "        No premable or explanation.\"\"\",\n",
    "        input_variables=[\"question\", \"document\"],\n",
    "    )    \n",
    "\n",
    "    state[\"grade\"] = robust_binary_grader(prompt=prompt, query=query, document=history_conversation)\n",
    "    settings.STEP += 1\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e457e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Node\n",
    "def decide_history_related_query(state: AgentState) -> str:\n",
    "    \"\"\" \n",
    "    If the query is related to the history conversation, but it is not self-contained. Go to rewrite node to augment the query.\n",
    "    If the query is not related to the history. Go to \"return_sorry\" node.\n",
    "    \"\"\"\n",
    "    if state['grade'][\"score\"] == \"yes\" and state['rewrite_counter'] < 3:\n",
    "        print(f\"The query is related to the history. But it is not self-contained. Let's re-write it.\\n\")\n",
    "        return \"rewrite\"\n",
    "    else:\n",
    "        print(f\"The query is not related to the history. I have nothing to do with it. \\n\")\n",
    "        return \"return_sorry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "history = settings.SHORT_TERM_MEMORY.get_history(session_id = 1)\n",
    "history.add_user_message(\"hi, there!\")\n",
    "history.add_ai_message(\"hi, how can I help you?\")\n",
    "history.add_user_message(\"My nasal is disconfort. Do you have a medicine to relieve sinus congestion and pressure?\")\n",
    "history.add_ai_message(\"phenylephrine is used to relieve nasal discomfort caused by colds, allergies, and hay fever. it is also used to relieve sinus congestion and pressure. phenylephrine will relieve symptoms but will not treat the cause of the symptoms or speed recovery. phenylephrine is in a class of medications called nasal decongestants. it works by reducing swelling of the blood vessels in the nasal passages.about Phenylephrine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f51e9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be related to history\n",
    "state = AgentState(session_id=1, query=\"How can I take it?\",wiki_used=False)\n",
    "\n",
    "state = grade_history_related_query(state)\n",
    "\n",
    "result = decide_history_related_query(state)\n",
    "\n",
    "print(f\"Decision: {result} \\n\")\n",
    "print(f\"State: {state} \\n\")\n",
    "# Related case\n",
    "state = AgentState(session_id=1, query=\"where can I buy it?\",wiki_used=False)\n",
    "\n",
    "state = grade_history_related_query(state)\n",
    "\n",
    "result = decide_history_related_query(state)\n",
    "\n",
    "print(f\"Decision: {result} \\n\")\n",
    "print(f\"State: {state} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d685757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be not related to history\n",
    "state = AgentState(session_id=1, query=\"Why doesn't my friend play tennis with me?\",wiki_used=False)\n",
    "\n",
    "state = grade_history_related_query(state)\n",
    "\n",
    "result = decide_history_related_query(state)\n",
    "\n",
    "print(f\"Decision: {result} \\n\")\n",
    "print(f\"State: {state} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f68d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action Node\n",
    "def rewrite_query(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Determine whether a query is related to history conversations.    \n",
    "    \"\"\"\n",
    "    query = state[\"query\"]\n",
    "    history = settings.SHORT_TERM_MEMORY.get_history(state[\"session_id\"])\n",
    "    history_conversation = history_as_text(history)\n",
    "\n",
    "    print(f\"===Step {settings.STEP}. The query is: {query}===\\n\")\n",
    "    print(\"I am rewriting the query so that I can retrieve relevant documents with a new query. \\n\")   \n",
    "    print(f\"history conversations: {history_conversation} \\n\")   \n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You are question re-writer that converts an input question to a better version that is optimized \\n \n",
    "        for vectorstore retrieval. Use the history conversation to resolve references. Keep the contextual meaning. \\n\n",
    "        Here is the history conversation: \\n\\n {document} \\n\\n\n",
    "        Here is the initial question: \\n\\n {question}. \\n\n",
    "        Improved question with no preamble.\"\"\",\n",
    "        input_variables=[\"question\", \"document\"],\n",
    "    )\n",
    "\n",
    "    new_query = robust_question_generater(prompt=prompt, query=query, document=history_conversation)\n",
    "    state['rewrite_counter'] += 1\n",
    "    state = AgentState(session_id=state[\"session_id\"], query=new_query, rewrite_counter=state['rewrite_counter']) # Avoid infinity loop in graph.\n",
    "    settings.STEP += 1\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should be related to history\n",
    "state = AgentState(session_id=1, query=\"How can I take it?\",wiki_used=False)\n",
    "\n",
    "state = rewrite_query(state)\n",
    "\n",
    "print(f\"State: {state} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1783ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action Node\n",
    "def return_without_docs(state: AgentState) -> AgentState:\n",
    "    \"\"\" \n",
    "    When the query has nothing to do with clinical topic or retrieval documents are not relevant to the query, \n",
    "    Then return 'sorry...' \n",
    "    \"\"\"\n",
    "    print(f\"===Step {settings.STEP} ===\\n\")\n",
    "    apology_sentences = [\n",
    "    \"I'm sorry, but I wasn't able to find any documents that match your request right now.\",\n",
    "    \"Apologies for the inconvenienceâ€”our system couldn't locate relevant information for that query.\",\n",
    "    \"I'm sorry, I couldn't retrieve the documents you're looking for at the moment.\",\n",
    "    \"I apologize that no relevant results were found. I'll keep improving to serve you better.\",\n",
    "    \"Sorry about that! I wasn't able to pull up the information you need this time.\"\n",
    "    ]\n",
    "\n",
    "    state[\"retrieved_doc\"] = random.choice(apology_sentences)\n",
    "    print(state[\"retrieved_doc\"])\n",
    "    settings.STEP += 1\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae3b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action Node\n",
    "def return_with_docs(state: AgentState) -> AgentState:\n",
    "    \"\"\" \n",
    "    When it successfully retrieved relevant documents, \n",
    "    Then return \n",
    "    \"\"\"\n",
    "    print(f\"===Step {settings.STEP} ===\\n\")        \n",
    "    print(\"I am happy to get what you need!\\n\")\n",
    "    settings.STEP += 1\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10f3e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action Node\n",
    "def save_to_memory(state: AgentState) -> AgentState:\n",
    "    \"\"\" \n",
    "    Before End, save user's query and final answer to memory \n",
    "    \"\"\"   \n",
    "    print(f\"===Step {settings.STEP} ===\\n\")\n",
    "    print(\"I am saving the user query and RAG response to memory.\\n\")  \n",
    "    print(f\"\"\"User query: {state[\"query\"]} - RAG response: {state[\"retrieved_doc\"]}\"\"\") \n",
    "    history = settings.SHORT_TERM_MEMORY.get_history(state[\"session_id\"])\n",
    "    history.add_user_message(state[\"query\"])\n",
    "    history.add_ai_message(state[\"retrieved_doc\"])\n",
    "    settings.STEP = 1 # Reset the STEP \n",
    "    return state    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c36dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action Node\n",
    "def retrieve(state: AgentState) -> AgentState:\n",
    "    \"\"\" \n",
    "    Retrieve documents by query.\n",
    "    Then grade the relevance.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"===Step {settings.STEP} ===\\n\")\n",
    "    print(\"I am seaching documents.\\n\")  \n",
    "    documents = rag.retrieve(state[\"query\"], top_k=3)\n",
    "    final_documents = [d.metadata[\"page_content\"] for d in documents if d.metadata[\"rerank_score\"] > 0.5]\n",
    "    state[\"retrieved_doc\"] = \". \".join(final_documents)\n",
    "    if len(final_documents) == 0:        \n",
    "        state[\"grade\"].docrelevant = {\"score\": \"no\"}\n",
    "    else:\n",
    "        state[\"grade\"].docrelevant = {\"score\": \"yes\"}\n",
    "\n",
    "    settings.STEP += 1\n",
    "\n",
    "    return state\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09fbaed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Decision Node\n",
    "def decide_relevant_docs(state: AgentState) -> str:\n",
    "    \"\"\" \n",
    "    If it retrieved relevant documents from RAG, go to \"return_with_docs\" node\n",
    "    If it didn't find anything from RAG and Wiki tool has not been used, then go to \"wiki_search\" tool node.\n",
    "    If it didn't find anything from RAG and Wiki, the return sorry.\n",
    "    \"\"\"\n",
    "    if state[\"grade\"].docrelevant[\"score\"] == \"yes\":\n",
    "        print(f\"I found some documents you may need.\\n\")\n",
    "        return \"return_with_docs\"\n",
    "    elif not state[\"wiki_used\"]:\n",
    "        print(\"I am sorry I didn't get the relevant document from RAG. I am going to search on wikipedia.\\n\")          \n",
    "        return \"wiki_search\"\n",
    "    else:\n",
    "        print(\"I am sorry I didn't get the relevant document from RAG and wikipedia.\\n\")         \n",
    "        return \"return_sorry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1a98bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Action Node\n",
    "def wiki_search(state: AgentState) -> AgentState:\n",
    "    \"\"\" \n",
    "    Search documents by Wikipedia seach tool.\n",
    "    Then grade the relevance.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"===Step {settings.STEP} ===\\n\")\n",
    "    print(\"I am seaching documents from Wikipedia.\\n\")  \n",
    "    documents = wiki.invoke({\"query\": state[\"query\"]})\n",
    "    json_list = wiki_to_json(documents)\n",
    "    \n",
    "    # Rank the wiki docs with crossEncoder\n",
    "    pairs = [[state[\"query\"], s[\"Summary\"]] for s in json_list]\n",
    "    scores = cross_encoder.predict(pairs, batch_size=32)\n",
    "    for j_l, score in zip(json_list, scores):\n",
    "        j_l[\"score\"] = float(score)\n",
    "\n",
    "    final_documents = [d[\"Summary\"] for d in json_list if d[\"score\"] > 0.5]\n",
    "    state[\"retrieved_doc\"] = \". \".join(final_documents)\n",
    "    state[\"wiki_used\"] = True # For one good query, only use wiki search once. Avoid infinity loop.\n",
    "    if len(final_documents) == 0:        \n",
    "        state[\"grade\"].docrelevant = {\"score\": \"no\"}\n",
    "    else:        \n",
    "        state[\"grade\"].docrelevant = {\"score\": \"yes\"}\n",
    "\n",
    "    settings.STEP += 1\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5297358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define graph\n",
    "agentic_rag_graph = StateGraph(AgentState)\n",
    "# Add nodes\n",
    "agentic_rag_graph.add_node(\"grade_selfcontained_node\", grade_selfcontained_query)\n",
    "\n",
    "#agentic_rag_graph.add_node(\"decide_selfcontained_node\", decide_selfcontained_query)\n",
    "\n",
    "agentic_rag_graph.add_node(\"grade_history_related_node\", grade_history_related_query)\n",
    "\n",
    "#agentic_rag_graph.add_node(\"decide_history_related_node\", decide_history_related_query)\n",
    "\n",
    "agentic_rag_graph.add_node(\"rewrite_query_node\", rewrite_query)\n",
    "\n",
    "agentic_rag_graph.add_node(\"grade_clinical_node\", grade_clinical_query)\n",
    "\n",
    "#agentic_rag_graph.add_node(\"decide_clinical_node\", decide_clinical_query)\n",
    "\n",
    "agentic_rag_graph.add_node(\"retrieve_node\", retrieve)\n",
    "\n",
    "agentic_rag_graph.add_node(\"decide_relevant_router\", lambda state:state) # Transparent\n",
    "\n",
    "agentic_rag_graph.add_node(\"return_sorry_node\", return_without_docs)\n",
    "\n",
    "agentic_rag_graph.add_node(\"return_docs_node\", return_with_docs)\n",
    "\n",
    "agentic_rag_graph.add_node(\"save_node\", save_to_memory)\n",
    "\n",
    "agentic_rag_graph.add_node(\"wiki_search_node\", wiki_search)\n",
    "\n",
    "# Add Edges\n",
    "\n",
    "agentic_rag_graph.add_edge(START, \"grade_selfcontained_node\")\n",
    "\n",
    "agentic_rag_graph.add_conditional_edges(\n",
    "    source=\"grade_selfcontained_node\",\n",
    "    path=decide_selfcontained_query,\n",
    "    path_map={\n",
    "        \"grade_clinical\": \"grade_clinical_node\",\n",
    "        \"grade_related_history\": \"grade_history_related_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "agentic_rag_graph.add_conditional_edges(\n",
    "    source=\"grade_history_related_node\",\n",
    "    path=decide_history_related_query,\n",
    "    path_map={\n",
    "        \"rewrite\": \"rewrite_query_node\",\n",
    "        \"return_sorry\": \"return_sorry_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "agentic_rag_graph.add_conditional_edges(\n",
    "    source=\"grade_clinical_node\",\n",
    "    path=decide_clinical_query,\n",
    "    path_map={\n",
    "        \"retrieve\": \"retrieve_node\",\n",
    "        \"return_sorry\": \"return_sorry_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "agentic_rag_graph.add_edge(\"rewrite_query_node\", \"grade_selfcontained_node\")\n",
    "\n",
    "agentic_rag_graph.add_edge(\"retrieve_node\", \"decide_relevant_router\")\n",
    "\n",
    "agentic_rag_graph.add_conditional_edges(\n",
    "    source=\"decide_relevant_router\",\n",
    "    path=decide_relevant_docs,\n",
    "    path_map={\n",
    "        \"return_sorry\": \"return_sorry_node\",\n",
    "        \"return_with_docs\": \"return_docs_node\",\n",
    "        \"wiki_search\": \"wiki_search_node\"\n",
    "    }\n",
    ")\n",
    "\n",
    "agentic_rag_graph.add_edge(\"wiki_search_node\", \"decide_relevant_router\")\n",
    "\n",
    "agentic_rag_graph.add_edge(\"return_sorry_node\", \"save_node\")\n",
    "\n",
    "agentic_rag_graph.add_edge(\"return_docs_node\", \"save_node\")\n",
    "\n",
    "agentic_rag_graph.add_edge(\"save_node\", END)\n",
    "\n",
    "app = agentic_rag_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [    \n",
    "    \"Is there anything I can assist you with?\",    \n",
    "    \"Can I help you in any way, next?\",\n",
    "    \"Do you have any questions about this?\",  \n",
    "    \"Are you looking for any particular information?\",\n",
    "    \"Do you want me to go over anything again?\",\n",
    "    \"What more information do you want?\"    \n",
    "]\n",
    "\n",
    "user_input = input(\"I am a Medicine Agentic RAG. I can help you get medical and clinical documents. Just tell me what you need?\")\n",
    "\n",
    "while user_input.strip().lower() not in [\"end\", \"exit\"]:\n",
    "    query = AgentState(query=user_input, session_id=1,wiki_used=False,rewrite_counter=0)\n",
    "    result = app.invoke(query)\n",
    "    print(f\"result:{result}\")\n",
    "    user_input = input(random.choice(questions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
