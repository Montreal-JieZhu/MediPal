{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "783a234e",
   "metadata": {},
   "source": [
    "### In this section, I will build an Agentic RAG\n",
    "\n",
    "Now I have a basic RAG which can retrieve relevant medicine documents based on the query.\n",
    "\n",
    "But why I need an **Agentic RAG**?\n",
    "\n",
    "### Problem description:\n",
    "\n",
    "In real conversation, users can ask anything we can not predict ahead. \n",
    "\n",
    "For example:\n",
    "In the third turn the user really want to ask 'How do I take Phenylephrine?'\n",
    "\n",
    "But he types 'How do I take it?'. From the context, 'it' means 'Phenylephrine'.\n",
    "\n",
    "If we retrieve documents by query 'How do I take it?', we can get unrelevant document.  'How do I take Phenylephrine?' makes more sense.\n",
    "\n",
    "Other senarios:\n",
    "\n",
    "1. In first turn, a user just greet without any question.\n",
    "2. User ask a random question in the middle of conversation.\n",
    "3. .........\n",
    "\n",
    "### Analysis:\n",
    "\n",
    "The root problem is how to determine whether a query is a clinial/medical query and whether a query is related previous conversation.\n",
    "\n",
    "### Solution:\n",
    "\n",
    "#### To handle all those, I will put a local LLM as a master agent to determine what to do next based on different situation.\n",
    "#### So I will involve RAG, langgraph, memory, local LLM, websearch tool... working together to make the RAG to ReAct by itself.\n",
    "\n",
    "### Implementation:\n",
    "\n",
    "* I will involve an agent to decide what to do next based on the query and history conversation. \n",
    "* Then, the agent will execute the task and observe the result to decide again..... until get a proper result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac03e8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ST' from 'langgraph.graph' (c:\\Users\\Montr\\AI_Projects\\.venv\\Lib\\site-packages\\langgraph\\graph\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msettings\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypedDict, List\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StateGraph, ST\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulti_vector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiVectorRetriever\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'ST' from 'langgraph.graph' (c:\\Users\\Montr\\AI_Projects\\.venv\\Lib\\site-packages\\langgraph\\graph\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from mytools import best_dtype, best_device, login_huggingface\n",
    "import os\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "import uuid\n",
    "import settings\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory # Short-term Memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb9626d",
   "metadata": {},
   "source": [
    "#### As I mentioned previously:\n",
    "\n",
    "Bio-Medical-Llama-3-8B model is a specialized large language model designed for biomedical applications. It is finetuned from the meta-llama/Meta-Llama-3-8B-Instruct model using a custom dataset containing over 500,000 diverse entries. These entries include a mix of synthetic and manually curated data, ensuring high quality and broad coverage of biomedical topics.\n",
    "\n",
    "The model is trained to understand and generate text related to various biomedical fields, making it a valuable tool for researchers, clinicians, and other professionals in the biomedical domain.\n",
    "\n",
    "@misc{ContactDoctor_Bio-Medical-Llama-3-8B, author = ContactDoctor, title = {ContactDoctor-Bio-Medical: A High-Performance Biomedical Language Model}, year = {2024}, howpublished = {https://huggingface.co/ContactDoctor/Bio-Medical-Llama-3-8B}, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8cdc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ContactDoctor/Bio-Medical-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195419ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "login_huggingface() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78220075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a HuggingFace model. Inference it from local GPU.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    dtype = best_dtype(),\n",
    "    device_map={\"\":best_device()}, \n",
    "    low_cpu_mem_usage=True     \n",
    ")\n",
    "print(\"Load tokenizer and base model done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3ff620",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)                    # full architecture tree (long but useful)\n",
    "print(model.config)             # core hyperparameters (dims, layers, heads…)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da7c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pipeline = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,   \n",
    ")\n",
    "\n",
    "# Wrapper normal piple with huggingfacepipeline\n",
    "hug_pipeline = HuggingFacePipeline(pipeline=original_pipeline)\n",
    "\n",
    "master_agent = ChatHuggingFace(llm=hug_pipeline) # It is the brain of the whole system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e85d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a short-term memory\n",
    "\n",
    "class Short_Term_Memory():\n",
    "    def __init__(self) -> None: \n",
    "        \"\"\"Initialize the message container and current session id \"\"\"       \n",
    "        self.session_store: dict[int,BaseChatMessageHistory] = {}\n",
    "        self.current_session_id: int = 0\n",
    "\n",
    "    def get_history(self, session_id: int) -> BaseChatMessageHistory:    \n",
    "        \"\"\"return history messages by sessionId\"\"\"    \n",
    "        self.current_session_id = session_id\n",
    "        if session_id not in self.session_store:\n",
    "            self.session_store[session_id] = ChatMessageHistory()\n",
    "        return self.session_store[session_id]\n",
    "    \n",
    "    def get_current_history(self) -> BaseChatMessageHistory:\n",
    "        \"\"\"return history messages for current session\"\"\"\n",
    "        return self.get_history(self.current_session_id)\n",
    "    \n",
    "    def delete_history(self, session_id: int) -> bool:\n",
    "        \"\"\"delete history messages by sessionId\"\"\"\n",
    "        if session_id in self.session_store:\n",
    "            deleted = self.session_store.pop(session_id)\n",
    "            if deleted:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def delete_current_history(self) -> bool:\n",
    "        \"\"\"delete history messages for current session\"\"\"\n",
    "        return self.delete_history(self.current_session_id)\n",
    "    \n",
    "# Convert a history chat message to a string\n",
    "def history_as_text(history: BaseChatMessageHistory) -> str:\n",
    "    \"\"\"convert history messsages into a string\"\"\"\n",
    "    return \"\\n\".join([\n",
    "        f\"{m.type.upper()}: {m.content}\"   # e.g. \"HUMAN: …\" or \"AI: …\"\n",
    "        for m in history.messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205392e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of the graph.\n",
    "\n",
    "    Attributes:\n",
    "        session_id: current session id\n",
    "        query: user's query or augmented query\n",
    "        retrieved_doc: retrieval docment        \n",
    "        generation: LLM generation        \n",
    "    \"\"\"\n",
    "    session_id: int\n",
    "    query: str\n",
    "    retrieved_doc: str        \n",
    "    generation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a global short-term memory for all users\n",
    "settings.SHORT_TERM_MEMORY = Short_Term_Memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First turn: greeting\n",
    "query_1 = \"hi, there\"\n",
    "session_id_1 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503d637e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFT_QLoRA_Llama3_1B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
