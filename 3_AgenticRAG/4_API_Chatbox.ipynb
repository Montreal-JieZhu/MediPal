{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31218495",
   "metadata": {},
   "source": [
    "## MediPal -- Frontend -- API and Chatbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882e4567",
   "metadata": {},
   "source": [
    "### In this section\n",
    "* I built an API endpoint by fastapi so that we only needs to launch the heavy process once. Other frontend apps just need to interact with the api. \n",
    "* I also built an chatbox with Gradio which can interact user by voice and text message. \n",
    "\n",
    "![](../assets/screenshots/front-end.PNG \"\")\n",
    "\n",
    "Key techniques: whisper, gtts, fastapi, gradio, unicorn\n",
    "\n",
    "##### The hardest part of this section should be deploying the whole thing to a inference platform or Cloud platform. That is is other domain. I will dive into it in later notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e254c6f2",
   "metadata": {},
   "source": [
    "#### The API endpoint is built by fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee4453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from src.medipal import ask\n",
    "\n",
    "class Query(BaseModel):\n",
    "    query: str\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "def medipal_post_api(query: Query):\n",
    "    user_query = query.query\n",
    "    ai_response = ask(user_query)\n",
    "    return {\"message\": ai_response}\n",
    "\n",
    "@app.get(\"/ask\")\n",
    "def medipal_get_api(query: str):\n",
    "    ai_response = ask(query)\n",
    "    return {\"message\": ai_response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff1d87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, time\n",
    "import uvicorn\n",
    "\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 30000\n",
    "\n",
    "_config = uvicorn.Config(app, host=HOST, port=PORT, log_level=\"info\")\n",
    "_server = uvicorn.Server(_config)\n",
    "\n",
    "def _run():\n",
    "    # Runs its own asyncio loop in THIS thread, so it won't conflict with Jupyter's loop\n",
    "    _server.run()\n",
    "\n",
    "_server_thread = threading.Thread(target=_run, daemon=True)\n",
    "_server_thread.start()\n",
    "\n",
    "time.sleep(1)\n",
    "print(f\"âœ… FastAPI running at http://{HOST}:{PORT}\")\n",
    "print(\"To stop the server, run the 'STOP' cell below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679780f2",
   "metadata": {},
   "source": [
    "#### The chatbox was built by gradio, whisper and gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "import datetime\n",
    "from gtts import gTTS\n",
    "from faster_whisper import WhisperModel\n",
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:30000/ask\" # medipal is running under the api\n",
    "\n",
    "workspace_base_path = os.getcwd()\n",
    "audio_path = os.path.join(workspace_base_path, \"audio\") \n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "asr_model = WhisperModel(\"turbo\")\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"\n",
    "    convert a audio file to text(ASR)    \n",
    "    \"\"\"\n",
    "    if not audio_file:\n",
    "        return None       \n",
    "    segments, info  = asr_model.transcribe(audio_file, beam_size=5)\n",
    "    text = \"\".join(seg.text for seg in segments)\n",
    "    return text\n",
    "\n",
    "def generate_response(query: str):\n",
    "    \"\"\"\n",
    "    Call medipal api to generate the answer.   \n",
    "    \"\"\"\n",
    "    if not query:\n",
    "        return \"I didnâ€™t catch anything. Please try speaking or typing.\"\n",
    "      \n",
    "    payload = {\"query\": query}\n",
    "\n",
    "    # Send POST request\n",
    "    response = requests.post(url, json=payload)    \n",
    "    ai_message = None\n",
    "    if response.status_code == 200:\n",
    "        ai_message = response.json()[\"message\"]        \n",
    "    else:\n",
    "        print(\"Error:\", response.status_code, response.text)\n",
    "    return ai_message\n",
    "\n",
    "def synthesize_audio(text):\n",
    "    \"\"\"\n",
    "    Convert text to audio(TTS)    \n",
    "    \"\"\"\n",
    "    filename = f\"Response-{timestamp}.wav\"    \n",
    "    file_path = os.path.join(audio_path, filename) \n",
    "    # Text to Audio\n",
    "    tts = gTTS(text, lang='en')\n",
    "    tts.save(file_path)\n",
    "    return file_path    \n",
    "\n",
    "# ----------------------------\n",
    "# Core interaction function\n",
    "# ----------------------------\n",
    "def voice_assistant(audio, text, history_state, use_audio_first):\n",
    "    \"\"\"\n",
    "    audio: (sr, np.ndarray) or None\n",
    "    text: str or None\n",
    "    history_state: list[tuple[str, str]]\n",
    "    use_audio_first: bool â€“ if True, prefer audio when both provided\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Decide which input to use\n",
    "        chosen_text = None\n",
    "        if use_audio_first and audio is not None:\n",
    "            chosen_text = transcribe_audio(audio)\n",
    "        elif text:\n",
    "            chosen_text = text\n",
    "        elif audio is not None:\n",
    "            chosen_text = transcribe_audio(audio)\n",
    "\n",
    "        if not chosen_text:            \n",
    "            raise gr.Error(\"Please provide either a voice recording or a text prompt.\")\n",
    "\n",
    "        query = chosen_text.strip()        \n",
    "\n",
    "        # LLM response\n",
    "        reply = generate_response(query)\n",
    "\n",
    "        # Optional TTS\n",
    "        audio_out = synthesize_audio(reply)\n",
    "\n",
    "        # Update history for Chatbot\n",
    "        history_state = history_state or []\n",
    "        history_state.append((query, reply))\n",
    "\n",
    "        # Return: chatbot, text output, audio output, cleared text box, preserved history\n",
    "        return history_state, reply, audio_out, gr.update(value=\"\"), history_state\n",
    "\n",
    "    except Exception as e:\n",
    "        #logging.exception(\"Error in voice_assistant\")\n",
    "        # Show the error gracefully in the text output; leave history unchanged\n",
    "        return history_state, f\"Error: {e}\", None, gr.update(), history_state\n",
    "\n",
    "def clear_history():\n",
    "    return [], gr.update(value=\"\")\n",
    "\n",
    "# ----------------------------\n",
    "# Custom CSS (sleek look)\n",
    "# ----------------------------\n",
    "CSS = \"\"\"\n",
    "    .gradio-container {max-width: 1024px !important;}\n",
    "    #title {\n",
    "    text-align: center;\n",
    "    font-size: 1.75rem;\n",
    "    font-weight: 800;\n",
    "    letter-spacing: 0.3px;\n",
    "    }\n",
    "    #subtitle {\n",
    "    text-align: center;\n",
    "    color: #6b7280;\n",
    "    margin-top: -10px;\n",
    "    margin-bottom: 12px;\n",
    "    }\n",
    "    .card {\n",
    "    background: linear-gradient(180deg, rgba(255,255,255,0.75) 0%, rgba(250,250,250,0.75) 100%);\n",
    "    border: 1px solid rgba(0,0,0,0.06);\n",
    "    border-radius: 16px;\n",
    "    padding: 16px;\n",
    "    box-shadow: 0 8px 22px rgba(0,0,0,0.06);\n",
    "    }\n",
    "    .footer {\n",
    "    text-align: center;\n",
    "    color: #9CA3AF;\n",
    "    font-size: 0.875rem;\n",
    "    margin-top: 8px;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# UI\n",
    "# ----------------------------\n",
    "with gr.Blocks(theme=gr.themes.Soft(), css=CSS, fill_height=True) as demo:\n",
    "    gr.HTML('<div id=\"title\">MediPal</div>')\n",
    "    gr.HTML('<div id=\"subtitle\">Your AI friend for medical and clinical Q&A</div>')\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            with gr.Group(elem_classes=\"card\"):\n",
    "                gr.Markdown(\"### Input\")\n",
    "                use_audio_first = gr.Checkbox(\n",
    "                    value=True, label=\"Prefer voice if both provided\"\n",
    "                )\n",
    "                mic = gr.Audio(\n",
    "                    sources=[\"microphone\"],\n",
    "                    type=\"filepath\",\n",
    "                    label=\"ðŸŽ¤ Record your prompt\",\n",
    "                    waveform_options={\"show_controls\": True},\n",
    "                )\n",
    "                text_in = gr.Textbox(\n",
    "                    label=\"âŒ¨ï¸ Or type here\",\n",
    "                    placeholder=\"Ask a question or say something...\",\n",
    "                    lines=2\n",
    "                )\n",
    "                with gr.Row():\n",
    "                    send = gr.Button(\"Send\", variant=\"primary\")\n",
    "                    clear_btn = gr.Button(\"Clear History\")\n",
    "\n",
    "        with gr.Column(scale=3):\n",
    "            with gr.Group(elem_classes=\"card\"):\n",
    "                gr.Markdown(\"### Conversation\")\n",
    "                chat = gr.Chatbot(\n",
    "                    label=\"History\",\n",
    "                    height=420,\n",
    "                    show_copy_button=True,\n",
    "                    avatar_images=(None, None),  # plug paths if you want custom avatars\n",
    "                    bubble_full_width=False,\n",
    "                )\n",
    "\n",
    "            with gr.Group(elem_classes=\"card\"):\n",
    "                gr.Markdown(\"### Assistant Outputs\")\n",
    "                out_text = gr.Textbox(\n",
    "                    label=\"Assistant (text output)\", lines=4, interactive=False\n",
    "                )\n",
    "                out_audio = gr.Audio(\n",
    "                    label=\"Assistant (voice output)\", autoplay=True, interactive=False\n",
    "                )\n",
    "\n",
    "    history_state = gr.State([])\n",
    "\n",
    "    # Click -> process\n",
    "    send.click(\n",
    "        voice_assistant,\n",
    "        inputs=[mic, text_in, history_state, use_audio_first],\n",
    "        outputs=[chat, out_text, out_audio, text_in, history_state],\n",
    "        queue=True,\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    # Enter/submit on the textbox also triggers send\n",
    "    text_in.submit(\n",
    "        voice_assistant,\n",
    "        inputs=[mic, text_in, history_state, use_audio_first],\n",
    "        outputs=[chat, out_text, out_audio, text_in, history_state],\n",
    "        queue=True,\n",
    "        show_progress=True\n",
    "    )\n",
    "\n",
    "    # Clear\n",
    "    clear_btn.click(\n",
    "        clear_history,\n",
    "        inputs=None,\n",
    "        outputs=[chat, text_in],\n",
    "    )\n",
    "        \n",
    "def launch_chatbox():\n",
    "    demo.launch(server_port=30001) # I hard code the port here, you can change it\n",
    "\n",
    "__all__ = [\"launch_chatbox\"]\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    launch_chatbox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c7ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFT_QLoRA_Llama3_1B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
